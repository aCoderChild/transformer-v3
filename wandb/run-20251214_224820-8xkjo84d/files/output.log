Epoch 1:   0%|                                         | 0/56250 [00:00<?, ?it/s]/opt/miniconda3/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.
  warnings.warn(warn_msg)
Traceback (most recent call last):                                               
  File "/Users/maianhpham/Documents/MediTranslator/scripts/train.py", line 237, in <module>
    main()
    ~~~~^^
  File "/Users/maianhpham/Documents/MediTranslator/scripts/train.py", line 231, in main
    trainer.train()
    ~~~~~~~~~~~~~^^
  File "/Users/maianhpham/Documents/MediTranslator/src/training/trainer.py", line 323, in train
    train_loss = self.train_epoch(epoch)
  File "/Users/maianhpham/Documents/MediTranslator/src/training/trainer.py", line 195, in train_epoch
    loss.backward()
    ~~~~~~~~~~~~~^^
  File "/opt/miniconda3/lib/python3.13/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
    ~~~~~~~~~~~~~~~~~~~~~~~^
        self, gradient, retain_graph, create_graph, inputs=inputs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/opt/miniconda3/lib/python3.13/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
    ~~~~~~~~~~~~~~~~~~~~^
        tensors,
        ^^^^^^^^
    ...<5 lines>...
        accumulate_grad=True,
        ^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/opt/miniconda3/lib/python3.13/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        t_outputs, *args, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
    )  # Calls into the C++ engine to run the backward pass
    ^
RuntimeError: MPS backend out of memory (MPS allocated: 10.34 GiB, other allocations: 16.68 GiB, max allowed: 27.20 GiB). Tried to allocate 195.31 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).
