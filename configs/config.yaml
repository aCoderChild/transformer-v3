# Transformer Configuration for Machine Translation
# Main Problem: Vi-En Translation

# Data Configuration
data:
  src_lang: "vi"
  tgt_lang: "en"
  train_src: "data/raw/train.vi.txt"
  train_tgt: "data/raw/train.en.txt"
  # Validation files (if not exist, will auto-split from training)
  val_src: "data/raw/val.vi.txt"
  val_tgt: "data/raw/val.en.txt"
  val_split: 0.1  # 10% of training data for validation if val files don't exist
  test_src: "data/raw/public_test.vi.txt"
  test_tgt: "data/raw/public_test.en.txt"
  max_seq_length: 128
  min_seq_length: 1

# Vocabulary Configuration
vocab:
  src_vocab_size: 32000
  tgt_vocab_size: 32000
  share_vocab: false
  min_freq: 2
  special_tokens:
    pad: "<pad>"
    unk: "<unk>"
    bos: "<bos>"
    eos: "<eos>"

# Model Configuration
model:
  d_model: 512           # Model dimension
  n_heads: 8             # Number of attention heads
  n_encoder_layers: 6    # Number of encoder layers
  n_decoder_layers: 6    # Number of decoder layers
  d_ff: 2048            # Feed-forward dimension
  dropout: 0.1          # Dropout rate
  max_seq_length: 512   # Maximum sequence length for positional encoding
  
# Training Configuration
training:
  batch_size: 32
  epochs: 50
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  
  # Optimizer
  optimizer: "adamw"
  learning_rate: 0.0001
  weight_decay: 0.01
  betas: [0.9, 0.98]
  eps: 1.0e-9
  
  # Learning Rate Scheduler (Warmup)
  scheduler: "warmup"
  warmup_steps: 4000
  
  # Label Smoothing
  label_smoothing: 0.1
  
  # Checkpointing
  save_every: 1000
  eval_every: 500
  log_every: 100
  
  # Weights & Biases
  use_wandb: false  # Set to true to enable wandb logging
  
  # Early Stopping
  early_stopping_patience: 5
  
# Inference Configuration
inference:
  beam_size: 5
  max_decode_length: 128
  length_penalty: 0.6
  
# Paths
paths:
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  vocab_dir: "data/vocab"

# Device
device: "cuda"  # or "cpu"
seed: 42

# Weights & Biases
wandb:
  project: "nlp-transformer-mt"
  entity: null
